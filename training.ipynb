{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API's Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.1.99)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face transformers to download pretrained model and tokenizer\n",
    "import transformers\n",
    "# Hugging face datasets to download the dataset\n",
    "import datasets\n",
    "# Pytorch for tensor\n",
    "import torch\n",
    "# For ploting the graph\n",
    "import matplotlib.pyplot as plt\n",
    "# Basic arithmatic operations\n",
    "import numpy as np\n",
    "# To show the progress bar\n",
    "import tqdm\n",
    "# For data handling\n",
    "import itertools\n",
    "# For turning parallelism on and off\n",
    "import os\n",
    "# Specific functions from the libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Here we download the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model repository\n",
    "model_repo = 'google/mt5-base'\n",
    "# download mt5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "# download model\n",
    "model= AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
    "# puts model onto GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Here we will be defining the dataset and downloading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = load_dataset('alt')\n",
    "# split the dataset into train validation and test\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250105, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add language token mapping to the tokenizer\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'en' : '<en>',\n",
    "    'fil' : '<fil>',\n",
    "    'hi' : '<hi>',\n",
    "    'id' : '<id>',\n",
    "    'ja' : '<ja>', \n",
    "}\n",
    "# create a dict of the dict\n",
    "special_tokens = { 'additional_special_tokens': list(LANG_TOKEN_MAPPING.values()) }\n",
    "# add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "# resize the token embeddings layer to correct size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling\n",
    "Functions to handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes and numericalizes input string\n",
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "\n",
    "# tokenizes and numericalizes target string\n",
    "def encode_target_str(text, tokenizer, seq_len):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "# get all translations between all permutations of pairs of languages\n",
    "def get_all_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_length=20):\n",
    "  input_ids = []\n",
    "  target_ids = []\n",
    "  \n",
    "  langs = list(lang_token_map.keys())\n",
    "  for input_lang, target_lang in itertools.permutations(langs, 2):\n",
    "    input_text = translations[input_lang]\n",
    "    target_text = translations[target_lang]\n",
    "    \n",
    "    if input_text is None or target_text is None:\n",
    "        return None, None\n",
    "    \n",
    "    input_ids.append(encode_input_str(input_text, target_lang, tokenizer, seq_length, \n",
    "                                    lang_token_map))\n",
    "    \n",
    "    target_ids.append(encode_target_str(target_text, tokenizer, seq_length))\n",
    "  \n",
    "  return input_ids, target_ids\n",
    "\n",
    "# generator function\n",
    "def get_full_dataloader(dataset, lang_token_map, tokenizer, batch_size=32, num_workers=8):\n",
    "    # get translations from the dataset\n",
    "    dataset = train_dataset['translation']\n",
    "    # intialize array\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        # get translations for all permuations of languages\n",
    "        input_id, target_id = get_all_translation_data(example, lang_token_map, tokenizer)\n",
    "        # case where nothing is returned\n",
    "        if input_id is None or target_id is None:\n",
    "            continue\n",
    "        # add the list of target and inputs \n",
    "        list_of_dicts = list(map(lambda x, y: {'input_ids': x, 'target_ids': y}, input_id, target_id))\n",
    "        data = data + list_of_dicts\n",
    "    # load dataset into a dataloader\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    # return the dataloader\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes and numericalizes input string\n",
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "\n",
    "# tokenizes and numericalizes target string\n",
    "def encode_target_str(text, tokenizer, seq_len):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "# improvement would be do this for all permutations of languages \n",
    "# or at least more than once per example\n",
    "def format_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_length=20):\n",
    "  # choose 2 random languages for i/o\n",
    "  langs = list(lang_token_map.keys())\n",
    "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "  \n",
    "  if input_text is None or target_text is None:\n",
    "    return None, None\n",
    "  \n",
    "  input_ids = encode_input_str(input_text, target_lang, tokenizer, seq_length, \n",
    "                                lang_token_map)\n",
    "  \n",
    "  target_ids = encode_target_str(target_text, tokenizer, seq_length)\n",
    "  \n",
    "  return input_ids, target_ids\n",
    "\n",
    "# gets a random batch of translations b/w two languages\n",
    "def transform_batch(batch, lang_token_map, tokenizer, seq_length=20):\n",
    "  input_ids = []\n",
    "  target_ids = []\n",
    "  \n",
    "  for example in batch['translation']:\n",
    "      input_id, target_id = format_translation_data(example, lang_token_map, tokenizer)\n",
    "      \n",
    "      if input_id is not None:\n",
    "          input_ids.append(input_id)\n",
    "          target_ids.append(target_id)\n",
    "  \n",
    "  input_ids = torch.stack(input_ids).cuda()\n",
    "  target_ids = torch.stack(target_ids).cuda()\n",
    "  \n",
    "  return input_ids, target_ids\n",
    "\n",
    "# generator function\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  dataset = dataset.shuffle()\n",
    "  \n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "      batch = dataset[i:i+batch_size]\n",
    "      yield transform_batch(batch, lang_token_map, tokenizer)\n",
    "\n",
    "def get_dataloader(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  dataset = dataset.shuffle()\n",
    "  dataset = dataset.map(lambda batch: transform_batch(batch, lang_token_map, tokenizer), batched=True)\n",
    "  dataset.set_format(type='torch', columns=['input_ids', 'target_ids'])\n",
    "  data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "  \n",
    "  return data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5\n",
    "batch_size = 64\n",
    "learning_rate = 5e-3\n",
    "n_batches = np.ceil(len(train_dataset) * 20 / batch_size)\n",
    "total_steps = n_batches * EPOCHS\n",
    "print_freq = int(total_steps / 100)\n",
    "checkpoint_freq = int(total_steps / 33)\n",
    "n_warmup_steps = int(0.01 * total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 856 282\n"
     ]
    }
   ],
   "source": [
    "print(print_freq, checkpoint_freq, n_warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "schedular = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_i = []\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Turn parallelism on\n",
    "    os.environ[\"TOKENIZER_PARALLELISM\"] = \"true\"\n",
    "    # Randomize data order, need to figure out a faster way to do this\n",
    "    loader = get_full_dataloader(train_dataset, LANG_TOKEN_MAPPING, tokenizer, batch_size, num_workers=8)\n",
    "    # Turn parallelism off\n",
    "    os.environ[\"TOKENIZER_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    for i, batch in tqdm.tqdm(enumerate(loader), total = n_batches):\n",
    "        inputs, targets = batch['input_ids'].cuda(), batch['target_ids'].cuda()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass (computes outputs and loss)\n",
    "        output = model(input_ids=inputs, labels=targets)\n",
    "        loss = output.loss\n",
    "        # Back propagation (computes gradients)\n",
    "        loss.backward()\n",
    "        # Optimization and scheduling\n",
    "        optimizer.step()\n",
    "        # Adjust every 100 batches\n",
    "        if(i+1) % 250 == 0:\n",
    "            loss_i.append(loss.item())\n",
    "            schedular.step()\n",
    "        # prints training updates\n",
    "        if (i+1) % print_freq == 0:\n",
    "            print(f'Epoch: {epoch + 1}, Batch: {i+1}/{n_batches}, Loss: {loss.item()}, LR: {schedular.get_last_lr()[0]}')\n",
    "        \n",
    "        if (i + 1) % checkpoint_freq == 0:\n",
    "            test_loss = eval_model(model, test_dataset)\n",
    "            if(test_loss < best_loss):\n",
    "                print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "                torch.save(model.state_dict(), 'mt5_translator_best.pt')\n",
    "                best_loss = test_loss\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'mt5_translator_final2.pt')\n",
    "plt.plot(loss_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
