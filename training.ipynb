{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API's Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentencepiece datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face transformers to download pretrained model and tokenizer\n",
    "import transformers\n",
    "# Hugging face datasets to download the dataset\n",
    "import datasets\n",
    "# Pytorch for tensor\n",
    "import torch\n",
    "# For ploting the graph\n",
    "import matplotlib.pyplot as plt\n",
    "# Basic arithmatic operations\n",
    "import numpy as np\n",
    "# To show the progress bar\n",
    "import tqdm\n",
    "# For data handling\n",
    "import itertools\n",
    "# Specific functions from the libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Here we download the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model repository\n",
    "model_repo = 'google/mt5-base'\n",
    "# download mt5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "# download model\n",
    "model= AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
    "# puts model onto GPU\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Here we will be defining the dataset and downloading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = load_dataset('alt')\n",
    "# split the dataset into train validation and test\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add language token mapping to the tokenizer\n",
    "LANG_TOKEN_MAPPING = {\n",
    "    'en' : '<en>',\n",
    "    'fil' : '<fil>',\n",
    "    'hi' : '<hi>',\n",
    "    'id' : '<id>',\n",
    "    'ja' : '<ja>', \n",
    "}\n",
    "# create a dict of the dict\n",
    "special_tokens = { 'additional_special_tokens': list(LANG_TOKEN_MAPPING.values()) }\n",
    "# add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "# resize the token embeddings layer to correct size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling\n",
    "Functions to handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizes and numericalizes input string\n",
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "\n",
    "# tokenizes and numericalizes target string\n",
    "def encode_target_str(text, tokenizer, seq_len):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "# get all translations between all permutations of pairs of languages\n",
    "def get_all_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_length=20):\n",
    "  input_ids = []\n",
    "  target_ids = []\n",
    "  \n",
    "  langs = list(lang_token_map.keys())\n",
    "  for input_lang, target_lang in itertools.permutations(langs, 2):\n",
    "    input_text = translations[input_lang]\n",
    "    target_text = translations[target_lang]\n",
    "    \n",
    "    if input_text is None or target_text is None:\n",
    "        return None, None\n",
    "    \n",
    "    input_ids.append(encode_input_str(input_text, target_lang, tokenizer, seq_length, \n",
    "                                    lang_token_map))\n",
    "    \n",
    "    target_ids.append(encode_target_str(target_text, tokenizer, seq_length))\n",
    "  \n",
    "  return input_ids, target_ids\n",
    "\n",
    "# generator function\n",
    "def get_full_dataloader(dataset, lang_token_map, tokenizer, batch_size=32, num_workers=8):\n",
    "    # get translations from the dataset\n",
    "    dataset = train_dataset['translation']\n",
    "    # intialize array\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        # get translations for all permuations of languages\n",
    "        input_id, target_id = get_all_translation_data(example, lang_token_map, tokenizer)\n",
    "        # case where nothing is returned\n",
    "        if input_id is None or target_id is None:\n",
    "            continue\n",
    "        # add the list of target and inputs \n",
    "        list_of_dicts = list(map(lambda x, y: {'input_ids': x, 'target_ids': y}, input_id, target_id))\n",
    "        data = data + list_of_dicts\n",
    "    # load dataset into a dataloader\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    # return the dataloader\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5\n",
    "batch_size = 64\n",
    "learning_rate = 5e-3\n",
    "n_batches = np.ceil(len(train_dataset) / batch_size)\n",
    "total_steps = n_batches * EPOCHS\n",
    "print_freq = total_steps / 100\n",
    "checkpoint_freq = total_steps / 10\n",
    "n_warmup_steps = int(0.01 * total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "schedular = get_linear_schedule_with_warmup(optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_i = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Randomize data order, need to figure out a faster way to do this\n",
    "    loader = get_full_dataloader(train_dataset, LANG_TOKEN_MAPPING, tokenizer, batch_size, num_workers=8)\n",
    "    \n",
    "    for i, batch in tqdm.tqdm(enumerate(loader), total = n_batches):\n",
    "        inputs, targets = batch['input_ids'].cuda(), batch['target_ids'].cuda()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass (computes outputs and loss)\n",
    "        output = model(input_ids=inputs, labels=targets)\n",
    "        loss = output.loss\n",
    "        # Back propagation (computes gradients)\n",
    "        loss.backward()\n",
    "        # Optimization and scheduling\n",
    "        optimizer.step()\n",
    "        # Adjust every 100 batches\n",
    "        if(i+1) % 500 == 0:\n",
    "            loss_i.append(loss.item())\n",
    "            schedular.step()\n",
    "        # prints training updates\n",
    "        if (i+1) % print_freq == 0:\n",
    "            print(f'Epoch: {epoch + 1}, Batch: {i+1}/{n_batches}, Loss: {loss.item()}, LR: {schedular.get_last_lr()[0]}')\n",
    "        \n",
    "        if (i + 1) % checkpoint_freq == 0:\n",
    "            test_loss = eval_model(model, test_dataset)\n",
    "            print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "            torch.save(model.state_dict(), 'mt5_translator.pt')\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'mt5_translator_final.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
